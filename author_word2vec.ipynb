{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 之前的分析一直是词袋分析，没有把句子的先后顺序反应出来\n",
    "# 然后看了一些博客，paper，sentence2vec虽然也有，但不过是用word2vec的方法在句子层面上做了一遍\n",
    "# 也有一些基于word2vec+rnn的句子相似度模型，其实这个挺不错，但是我目前的tensorflow基础好像还做不到\n",
    "# 最终决定先做一个word2vec，然后用词向量的简单平均求和看看，顺便也复习一下tensorflow里怎么实现word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'C:/Users\\Administrator\\Desktop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(filepath+'train.csv')\n",
    "test_data = pd.read_csv(filepath+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_combine = pd.concat([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_combine['word_split'] = data_combine.text.apply(lambda x:word_tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for i in data_combine.word_split:\n",
    "    for x in i:\n",
    "        if x not in wordlist:\n",
    "            wordlist.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 建立两个字典\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "for i,i_index in enumerate(wordlist):\n",
    "    word2int[i]=i_index\n",
    "    int2word[i_index] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "batch_size = None\n",
    "win = 3\n",
    "epoch = 60000\n",
    "n=0\n",
    "pp=[[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(shape=[batch_size],dtype=tf.int32)\n",
    "    labels = tf.placeholder(shape=[batch_size,1],dtype=tf.int32)\n",
    "    embed_dict = tf.Variable(\n",
    "        tf.random_uniform(shape=[vocab_size,embed_size],minval=-1.0,maxval=1.0,name='vector')\n",
    "    )\n",
    "    nce_weight = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocab_size,embed_size],stddev=1.0/math.sqrt(embed_size))\n",
    "    )\n",
    "    nce_bias = tf.Variable(tf.zeros([vocab_size]))\n",
    "        \n",
    "    embed = tf.nn.embedding_lookup(embed_dict,inputs)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weight,biases=nce_bias,labels=labels,num_sampled=100,num_classes=vocab_size,inputs=embed)\n",
    "    )\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.04).minimize(loss)     \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(input_sentence,n=n):\n",
    "        batch_input = []\n",
    "        batch_label = []\n",
    "        sent_len = len(input_sentence)\n",
    "        for i in range(sent_len):\n",
    "            start = max(0,i-win)\n",
    "            end = min(sent_len,i+win)\n",
    "            for index in range(start,end):\n",
    "                if index==i:\n",
    "                    continue\n",
    "                else:\n",
    "                    input_id = int2word[input_sentence[i]]\n",
    "                    label_id = int2word[input_sentence[index]]\n",
    "                    batch_input.append(input_id)\n",
    "                    batch_label.append(label_id)\n",
    "        if len(batch_input)==0:\n",
    "            return\n",
    "        batch_input = np.array(batch_input,dtype=np.int32)\n",
    "        batch_label = np.array(batch_label,dtype=np.int32)\n",
    "        batch_label = np.reshape(batch_label,[len(batch_label),1])\n",
    "        feed_dic = {inputs:batch_input,labels:batch_label}\n",
    "        _,loss_val = sess.run([train_op,loss],feed_dict=feed_dic)\n",
    "        n+=1\n",
    "        if n%100==0:\n",
    "            pp[0].append(n)\n",
    "            pp[1].append(loss_val)\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "sess.run(init)\n",
    "with sess.as_default():\n",
    "    for j in range(2):\n",
    "        for i in data_combine.word_split:\n",
    "            if n<epoch:\n",
    "                n = train(i,n)\n",
    "    final_embeddings = embed_dict.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_embed = pd.DataFrame(final_embeddings,columns=['embed_'+str(i) for i in range(embed_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "def embed(x,num):\n",
    "    sum = 0\n",
    "    l = 0\n",
    "    for i in x:\n",
    "        index = int2word[i]\n",
    "        sum+=final_embed['embed_'+str(num)][index]\n",
    "        l+=1\n",
    "    return sum/l\n",
    "for i in range(embed_size):\n",
    "    data_combine['embed_mean'+str(i)] = data_combine.word_split.apply(lambda x:embed(x,i))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('D:/大学/大三上/数据挖掘/作业/数据embed.xlsx')\n",
    "data_combine.to_excel(writer,sheet_name='sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
